#!/usr/bin/env python3
"""
Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£Ù†ØµØ§Ø± Ø§Ù„Ù„Ù‡
====================

Ø³ÙƒØ±ÙŠØ¨Øª Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£Ù†ØµØ§Ø± Ø§Ù„Ù„Ù‡ Ø§Ù„ÙŠÙ…Ù†ÙŠ
"""

import requests
import asyncio
from bs4 import BeautifulSoup
from newspaper import Article as NewsArticle
from readability import Document
import html2text
from datetime import datetime
from ansarollah_config import AnsarallahConfig

class AnsarallahTester:
    """ÙØ¦Ø© Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£Ù†ØµØ§Ø± Ø§Ù„Ù„Ù‡"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def test_article_extraction(self, url: str) -> dict:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆÙ‰ Ù…Ù‚Ø§Ù„"""
        print(f"ğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù†: {url}")
        
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            print(f"âœ… ØªÙ… Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­ - Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response.status_code}")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„
            article_data = self.extract_article_data(url, response.text)
            
            return article_data
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‚Ø§Ù„: {e}")
            return {}
    
    def extract_article_data(self, url: str, html_content: str) -> dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…Ù† HTML"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        article_data = {
            'url': url,
            'title': '',
            'content': '',
            'summary': '',
            'author': '',
            'date': '',
            'image_url': '',
            'category': ''
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        print("\nğŸ“° Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†...")
        for selector in AnsarallahConfig.CONTENT_SELECTORS['title']:
            title_element = soup.select_one(selector)
            if title_element:
                article_data['title'] = title_element.get_text(strip=True)
                print(f"âœ… Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {article_data['title']}")
                break
        
        if not article_data['title']:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† title tag
            title_tag = soup.find('title')
            if title_tag:
                article_data['title'] = title_tag.get_text(strip=True)
                print(f"âš ï¸ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…Ù† title tag: {article_data['title']}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        print("\nğŸ“„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰...")
        for selector in AnsarallahConfig.CONTENT_SELECTORS['content']:
            content_element = soup.select_one(selector)
            if content_element:
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                for unwanted in content_element.find_all(['script', 'style', 'aside', 'nav']):
                    unwanted.decompose()
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ
                content_text = content_element.get_text(separator='\n', strip=True)
                article_data['content'] = content_text
                print(f"âœ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {len(content_text)} Ø­Ø±Ù")
                
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ HTML Ù„Ù„Ù€ markdown
                content_html = str(content_element)
                h = html2text.HTML2Text()
                h.ignore_links = False
                h.ignore_images = False
                article_data['content_markdown'] = h.handle(content_html)
                break
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®
        print("\nğŸ“… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®...")
        for selector in AnsarallahConfig.CONTENT_SELECTORS['date']:
            date_element = soup.select_one(selector)
            if date_element:
                article_data['date'] = date_element.get_text(strip=True)
                print(f"âœ… Ø§Ù„ØªØ§Ø±ÙŠØ®: {article_data['date']}")
                break
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØ§ØªØ¨
        print("\nâœï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØ§ØªØ¨...")
        for selector in AnsarallahConfig.CONTENT_SELECTORS['author']:
            author_element = soup.select_one(selector)
            if author_element:
                article_data['author'] = author_element.get_text(strip=True)
                print(f"âœ… Ø§Ù„ÙƒØ§ØªØ¨: {article_data['author']}")
                break
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø©
        print("\nğŸ–¼ï¸ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø©...")
        for selector in AnsarallahConfig.CONTENT_SELECTORS['image']:
            image_element = soup.select_one(selector)
            if image_element:
                image_url = image_element.get('src')
                if image_url:
                    if image_url.startswith('http'):
                        article_data['image_url'] = image_url
                    else:
                        article_data['image_url'] = f"https://www.ansarollah.com.ye{image_url}"
                    print(f"âœ… Ø§Ù„ØµÙˆØ±Ø©: {article_data['image_url']}")
                    break
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ø³Ù…
        print("\nğŸ“‚ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ø³Ù…...")
        for selector in AnsarallahConfig.CONTENT_SELECTORS['category']:
            category_element = soup.select_one(selector)
            if category_element:
                article_data['category'] = category_element.get_text(strip=True)
                print(f"âœ… Ø§Ù„Ù‚Ø³Ù…: {article_data['category']}")
                break
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ
        if article_data['content']:
            summary = self.create_summary(article_data['content'])
            article_data['summary'] = summary
            print(f"âœ… Ø§Ù„Ù…Ù„Ø®Øµ: {len(summary)} Ø­Ø±Ù")
        
        return article_data
    
    def create_summary(self, content: str, max_length: int = 300) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        content = content.replace('\n', ' ').strip()
        
        if len(content) <= max_length:
            return content
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¢Ø®Ø± Ø¬Ù…Ù„Ø© ÙƒØ§Ù…Ù„Ø©
        truncated = content[:max_length]
        last_sentence_end = max(
            truncated.rfind('.'),
            truncated.rfind('!'),
            truncated.rfind('?')
        )
        
        if last_sentence_end > max_length * 0.7:
            return content[:last_sentence_end + 1]
        else:
            return content[:max_length] + "..."
    
    def test_newspaper3k(self, url: str) -> dict:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… newspaper3k"""
        print(f"\nğŸ“° Ø§Ø®ØªØ¨Ø§Ø± newspaper3k Ø¹Ù„Ù‰: {url}")
        
        try:
            article = NewsArticle(url)
            article.download()
            article.parse()
            
            result = {
                'title': article.title,
                'content': article.text,
                'authors': article.authors,
                'publish_date': article.publish_date,
                'top_image': article.top_image,
                'keywords': list(article.keywords)
            }
            
            print(f"âœ… newspaper3k - Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {result['title']}")
            print(f"âœ… newspaper3k - Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {len(result['content'])} Ø­Ø±Ù")
            print(f"âœ… newspaper3k - Ø§Ù„ØµÙˆØ±Ø©: {result['top_image']}")
            
            return result
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ newspaper3k: {e}")
            return {}
    
    def test_readability(self, url: str) -> dict:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… readability"""
        print(f"\nğŸ“– Ø§Ø®ØªØ¨Ø§Ø± readability Ø¹Ù„Ù‰: {url}")
        
        try:
            response = self.session.get(url, timeout=30)
            doc = Document(response.text)
            
            result = {
                'title': doc.title(),
                'content': doc.summary(),
                'clean_content': BeautifulSoup(doc.summary(), 'html.parser').get_text()
            }
            
            print(f"âœ… readability - Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {result['title']}")
            print(f"âœ… readability - Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {len(result['clean_content'])} Ø­Ø±Ù")
            
            return result
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ readability: {e}")
            return {}
    
    def compare_methods(self, url: str):
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ø·Ø±Ù‚ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
        print("=" * 80)
        print("ğŸ”¬ Ù…Ù‚Ø§Ø±Ù†Ø© Ø·Ø±Ù‚ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬")
        print("=" * 80)
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø®ØµØµØ©
        custom_result = self.test_article_extraction(url)
        
        # newspaper3k
        newspaper_result = self.test_newspaper3k(url)
        
        # readability
        readability_result = self.test_readability(url)
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print("\n" + "=" * 80)
        print("ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        print("=" * 80)
        
        print(f"\nğŸ”§ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø®ØµØµØ©:")
        print(f"  Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {custom_result.get('title', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')}")
        print(f"  Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {len(custom_result.get('content', ''))} Ø­Ø±Ù")
        print(f"  Ø§Ù„ØµÙˆØ±Ø©: {'âœ… Ù…ØªÙˆÙØ±Ø©' if custom_result.get('image_url') else 'âŒ ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©'}")
        
        print(f"\nğŸ“° Newspaper3k:")
        print(f"  Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {newspaper_result.get('title', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')}")
        print(f"  Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {len(newspaper_result.get('content', ''))} Ø­Ø±Ù")
        print(f"  Ø§Ù„ØµÙˆØ±Ø©: {'âœ… Ù…ØªÙˆÙØ±Ø©' if newspaper_result.get('top_image') else 'âŒ ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©'}")
        
        print(f"\nğŸ“– Readability:")
        print(f"  Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {readability_result.get('title', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')}")
        print(f"  Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {len(readability_result.get('clean_content', ''))} Ø­Ø±Ù")
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙØ¶Ù„
        print(f"\nğŸ† Ø§Ù„ØªÙˆØµÙŠØ©:")
        if len(custom_result.get('content', '')) > len(newspaper_result.get('content', '')):
            print("  Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø®ØµØµØ© ØªØ¹Ø·ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø£ÙØ¶Ù„")
        else:
            print("  newspaper3k ÙŠØ¹Ø·ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø£ÙØ¶Ù„")
        
        return {
            'custom': custom_result,
            'newspaper': newspaper_result,
            'readability': readability_result
        }

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("ğŸ¤– Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£Ù†ØµØ§Ø± Ø§Ù„Ù„Ù‡ Ø§Ù„ÙŠÙ…Ù†ÙŠ")
    print("=" * 80)
    
    # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø§Ø®ØªØ¨Ø§Ø±Ù‡
    test_url = "https://www.ansarollah.com.ye/archives/801488"
    
    tester = AnsarallahTester()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    results = tester.compare_methods(test_url)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ø§Ù„Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    if results['custom']:
        print("\n" + "=" * 80)
        print("ğŸ“± Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø©")
        print("=" * 80)
        
        message = AnsarallahConfig.format_message(results['custom'])
        print(message)
    
    print("\nâœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!")

if __name__ == "__main__":
    main()